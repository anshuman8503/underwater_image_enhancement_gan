from google.colab import drive
drive.mount('/content/drive')

!pip install tensorflow
!pip install keras

# Step 2: Import Libraries
import tensorflow as tf
from tensorflow import keras
from keras.models import load_model
from keras import layers, Model
import numpy as np
import os
from glob import glob
from keras.preprocessing.image import load_img, img_to_array
import matplotlib.pyplot as plt
from tensorflow.keras.applications import VGG19
from tensorflow.keras.models import Model
from skimage.metrics import peak_signal_noise_ratio as psnr
from skimage.metrics import structural_similarity as ssim

tf.keras.backend.clear_session()

class InstanceNormalization(layers.Layer):
    def __init__(self, epsilon=1e-5):
        super(InstanceNormalization, self).__init__()
        self.epsilon = epsilon

    def build(self, input_shape):
        self.scale = self.add_weight(name='scale',
                                     shape=(input_shape[-1],),
                                     initializer='ones',
                                     trainable=True)
        self.offset = self.add_weight(name='offset',
                                      shape=(input_shape[-1],),
                                      initializer='zeros',
                                      trainable=True)

    def call(self, inputs):
        mean, variance = tf.nn.moments(inputs, axes=[1, 2], keepdims=True)
        return self.scale * (inputs - mean) / tf.sqrt(variance + self.epsilon) + self.offset

# Generator (U-Net with residual blocks and InstanceNorm)
def build_generator(input_shape=(256, 256, 3)):
    def res_block(x, filters):
        shortcut = x
        x = layers.Conv2D(filters, 3, padding='same')(x)
        x = InstanceNormalization()(x)
        x = layers.ReLU()(x)
        x = layers.Conv2D(filters, 3, padding='same')(x)
        x = InstanceNormalization()(x)
        x = layers.Add()([shortcut, x])
        return x

    inputs = layers.Input(shape=input_shape)
    x = layers.Conv2D(64, 7, padding='same')(inputs)
    x = InstanceNormalization()(x)
    x = layers.ReLU()(x)

    # Downsampling
    x = layers.Conv2D(128, 3, strides=2, padding='same')(x)
    x = InstanceNormalization()(x)
    x = layers.ReLU()(x)
    x = layers.Conv2D(256, 3, strides=2, padding='same')(x)
    x = InstanceNormalization()(x)
    x = layers.ReLU()(x)

    # Residual blocks
    for _ in range(6):
        x = res_block(x, 256)

    # Upsampling
    x = layers.Conv2DTranspose(128, 3, strides=2, padding='same')(x)
    x = InstanceNormalization()(x)
    x = layers.ReLU()(x)
    x = layers.Conv2DTranspose(64, 3, strides=2, padding='same')(x)
    x = InstanceNormalization()(x)
    x = layers.ReLU()(x)

    outputs = layers.Conv2D(3, 7, padding='same', activation='tanh')(x)
    return Model(inputs, outputs, name="Generator")

# Discriminator (PatchGAN)
def build_discriminator(input_shape=(256, 256, 3)):
    inputs = layers.Input(shape=input_shape)
    x = layers.Conv2D(64, 4, strides=2, padding='same')(inputs)
    x = layers.LeakyReLU(0.2)(x)
    for filters in [128, 256, 512]:
        x = layers.Conv2D(filters, 4, strides=2, padding='same')(x)
        x = layers.LeakyReLU(0.2)(x)
    x = layers.Conv2D(1, 4, padding='same')(x)
    return Model(inputs, x, name="Discriminator")

# VGG-based perceptual loss
def build_vgg_loss_model():
    vgg = VGG19(include_top=False, weights='imagenet', input_shape=(256, 256, 3))
    vgg.trainable = False
    return Model(vgg.input, outputs=[vgg.get_layer('block1_conv2').output, vgg.get_layer('block2_conv2').output, vgg.get_layer('block3_conv3').output, vgg.get_layer('block5_conv2').output])

# Losses
bce = tf.keras.losses.BinaryCrossentropy(from_logits=True)
vgg_model = build_vgg_loss_model()

def perceptual_loss(y_true, y_pred):
    y_true_features = vgg_model(y_true)
    y_pred_features = vgg_model(y_pred)
    loss = (
        1.2 * tf.reduce_mean(tf.abs(y_true_features[0] - y_pred_features[0])) +  # block1_conv2
        1.0 * tf.reduce_mean(tf.abs(y_true_features[1] - y_pred_features[1])) +  # block2_conv2
        0.8 * tf.reduce_mean(tf.abs(y_true_features[2] - y_pred_features[2])) +  # block3_conv3
        0.8 * tf.reduce_mean(tf.abs(y_true_features[3] - y_pred_features[3]))    # block5_conv2
    )
    return loss

def generator_loss(disc_generated_output, gen_output, target):
    adv_loss = bce(tf.ones_like(disc_generated_output), disc_generated_output)
    l1_loss = tf.reduce_mean(tf.abs(target - gen_output))
    perc_loss = perceptual_loss(target, gen_output)
    return adv_loss + 100 * l1_loss + 10 * perc_loss

def discriminator_loss(real_output, fake_output):
    real_loss = bce(tf.ones_like(real_output), real_output)
    fake_loss = bce(tf.zeros_like(fake_output), fake_output)
    return real_loss + fake_loss



def load_images(path):
    images = []
    extensions = ["*.png", "*.jpg", "*.jpeg"]
    files = []

    for ext in extensions:
        files.extend(glob(os.path.join(path, ext)))

    files = sorted(files)  # Optional: ensures consistent order

    for file in files:
        img = load_img(file, target_size=(256, 256))
        img = img_to_array(img) / 127.5 - 1.0  # Normalize to [-1, 1]
        images.append(img)

    return np.array(images)

# def augment(input_image, target_image):
#     # Random flip
#     seed = tf.random.uniform(shape=[2], maxval=1000, dtype=tf.int32)

#     input_image = tf.image.stateless_random_flip_left_right(input_image, seed)
#     target_image = tf.image.stateless_random_flip_left_right(target_image, seed)

#     # Random brightness, contrast, saturation
#     input_image = tf.image.stateless_random_brightness(input_image, max_delta=0.1, seed=seed)
#     # target_image = tf.image.stateless_random_brightness(target_image, max_delta=0.1, seed=seed + 1)

#     input_image = tf.image.stateless_random_contrast(input_image, lower=0.9, upper=1.1, seed=seed + 2)
#     # target_image = tf.image.stateless_random_contrast(target_image, lower=0.9, upper=1.1, seed=seed + 3)

#     input_image = tf.image.stateless_random_saturation(input_image, lower=0.95, upper=1.05, seed=seed + 4)
#     # target_image = tf.image.stateless_random_saturation(target_image, lower=0.95, upper=1.05, seed=seed + 5)

#     return input_image, target_image

# Prepare dataset
input_path = "/content/drive/MyDrive/Underwater_Image_Enhancement/augmented-raw-890"
target_path = "/content/drive/MyDrive/Underwater_Image_Enhancement/augmented-reference-890"

input_images = load_images(input_path)
target_images = load_images(target_path)

BATCH_SIZE = 8
dataset = tf.data.Dataset.from_tensor_slices((input_images, target_images))
AUTOTUNE = tf.data.AUTOTUNE
dataset = dataset.shuffle(100).batch(BATCH_SIZE).prefetch(AUTOTUNE)

# Build models
generator = build_generator()
discriminator = build_discriminator()
gen_optimizer = tf.keras.optimizers.Adam(3e-4, beta_1=0.5)
disc_optimizer = tf.keras.optimizers.Adam(1.5e-4, beta_1=0.5)

# Training step
@tf.function
def train_step(input_image, target):
    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:
        gen_output = generator(input_image, training=True)
        disc_real = discriminator(target, training=True)
        disc_fake = discriminator(gen_output, training=True)

        gen_loss = generator_loss(disc_fake, gen_output, target)
        disc_loss = discriminator_loss(disc_real, disc_fake)

    gradients_gen = gen_tape.gradient(gen_loss, generator.trainable_variables)
    gradients_disc = disc_tape.gradient(disc_loss, discriminator.trainable_variables)
    gen_optimizer.apply_gradients(zip(gradients_gen, generator.trainable_variables))
    disc_optimizer.apply_gradients(zip(gradients_disc, discriminator.trainable_variables))

    return gen_loss, disc_loss

# Test & visualize
test_path = "/content/drive/MyDrive/Underwater_Image_Enhancement/Inp"
test_images = load_images(test_path)
# predictions = generator.predict(test_images)

EPOCHS = 100

# Create directory for checkpoints
checkpoint_dir = "/content/drive/MyDrive/Underwater_Image_Enhancement/Checkpoints"
os.makedirs(checkpoint_dir, exist_ok=True)

for epoch in range(EPOCHS):
    print(f"\nEpoch {epoch + 1}/{EPOCHS}")
    g_loss_avg, d_loss_avg = 0.0, 0.0
    count = 0

    for inp, tar in dataset:
        g_loss, d_loss = train_step(inp, tar)
        g_loss_avg += g_loss
        d_loss_avg += d_loss
        count += 1

    g_loss_avg /= count
    d_loss_avg /= count
    print(f"Generator Loss: {g_loss_avg:.4f} | Discriminator Loss: {d_loss_avg:.4f}")

    # Save model checkpoints every 10 epochs
    if (epoch + 1) % 10 == 0:
        gen_ckpt_path = os.path.join(checkpoint_dir, f"generator_epoch_{epoch+1}.keras")
        disc_ckpt_path = os.path.join(checkpoint_dir, f"discriminator_epoch_{epoch+1}.keras")
        generator.save(gen_ckpt_path)
        discriminator.save(disc_ckpt_path)
        print(f"‚úîÔ∏è Saved checkpoint at epoch {epoch + 1}")

    # Visualize progress every 5 epochs
    if (epoch + 1) % 5 == 0:
        print("üîç Generating test output for visual inspection and evaluation...")
        sample_preds = generator.predict(test_images[:5])
        sample_preds = np.clip(sample_preds, -1.0, 1.0)  # Ensure valid range
        sample_preds = ((sample_preds + 1) / 2).astype(np.float32)
        test_inputs = ((test_images[:5] + 1) / 2).astype(np.float32)
        test_target_path = "/content/drive/MyDrive/Underwater_Image_Enhancement/GTr"  # or whatever the correct path is
        test_targets = load_images(test_target_path)


        # Compute PSNR and SSIM
        psnr_scores = []
        ssim_scores = []
        for i in range(5):
            psnr = tf.image.psnr(test_targets[i], sample_preds[i], max_val=1.0).numpy()
            ssim = tf.image.ssim(test_targets[i], sample_preds[i], max_val=1.0).numpy()
            psnr_scores.append(psnr)
            ssim_scores.append(ssim)

        print(f"üìà Average PSNR: {np.mean(psnr_scores):.4f}")
        print(f"üìà Average SSIM: {np.mean(ssim_scores):.4f}")

        # Plotting Input vs Enhanced
        plt.figure(figsize=(15, 6))
        for i in range(5):
            plt.subplot(2, 5, i + 1)
            plt.imshow(test_inputs[i])
            plt.axis("off")
            plt.title("Input")

            plt.subplot(2, 5, i + 6)
            plt.imshow(sample_preds[i])
            plt.axis("off")
            plt.title("Enhanced")
        plt.tight_layout()
        plt.show()

# Save models
generator.save("/content/drive/MyDrive/Underwater_Image_Enhancement/generator_model.keras")
discriminator.save("/content/drive/MyDrive/Underwater_Image_Enhancement/discriminator_model.keras")

# Test & visualize
test_path = "/content/drive/MyDrive/Underwater_Image_Enhancement/Inp"
test_images = load_images(test_path)
predictions = generator.predict(test_images)

from skimage.metrics import peak_signal_noise_ratio as psnr
from skimage.metrics import structural_similarity as ssim

# Load reference images (ground truth for test images)
ref_path = "/content/drive/MyDrive/Underwater_Image_Enhancement/GTr"
ref_images = load_images(ref_path)

# Normalize predictions and references to [0, 1]
predictions = ((predictions + 1) / 2).astype(np.float32)
ref_images = ((ref_images + 1) / 2).astype(np.float32)

# Compute metrics
psnr_scores = []
ssim_scores = []

for i in range(len(predictions)):
    psnr_val = psnr(ref_images[i], predictions[i], data_range=1.0)
    ssim_val = ssim(ref_images[i], predictions[i], channel_axis=-1, data_range=1.0)
    psnr_scores.append(psnr_val)
    ssim_scores.append(ssim_val)

# Print average results
print(f"\n--- Evaluation Metrics ---")
print(f"Average PSNR: {np.mean(psnr_scores):.2f} dB")
print(f"Average SSIM: {np.mean(ssim_scores):.4f}")

!pip install lpips

import lpips
import torch
import numpy as np
from PIL import Image
import torchvision.transforms as transforms

# Initialize the LPIPS model
lpips_fn = lpips.LPIPS(net='vgg')  # Options: 'alex', 'vgg', 'squeeze'

# Convert numpy image [H, W, C] to torch tensor
def preprocess(img_np):
    transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Resize((img_np.shape[0], img_np.shape[1])),  # maintain size
        transforms.Normalize(mean=[0.5]*3, std=[0.5]*3)  # Normalize to [-1, 1]
    ])
    img_tensor = transform(Image.fromarray((img_np * 255).astype(np.uint8))).unsqueeze(0)
    return img_tensor

# Evaluate LPIPS between two batches of images
lpips_scores = []
for i in range(len(predictions)):
    img1 = preprocess(ref_images[i])
    img2 = preprocess(predictions[i])

    with torch.no_grad():
        dist = lpips_fn(img1, img2)
    lpips_scores.append(dist.item())

print(f"üìä Average LPIPS: {np.mean(lpips_scores):.4f}")
